---
title: GARCH Model Implementation
author:
  - name: Krzysztof Wojdalski
  - name: Piotr Jagiełło
  - name: Shah Syed
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    highlight-style: github
execute:
  echo: true
  warning: false
jupyter: python3
---


# Introduction

## Background
The primary objective of this project is to replicate the GARCH model analysis presented in Robert Engle's 2001 Nobel Prize lecture, "GARCH 101: The Use of ARCH/GARCH Models in Applied Econometrics." The lecture illustrates the application of GARCH models in analyzing and forecasting financial market volatility. To enhance the study, we incorporate several extensions: first, we develop our own GARCH(1,1) implementation; second, we compare GARCH(1,1) results with alternative volatility models, specifically E-GARCH and GJR-GARCH, using the same dataset as the original paper; and finally, we investigate the evolution of market volatility since the paper's publication.

### Rationale behind the Study

1) The article's data requirements are readily accessible and obtainable
2) The author's established reputation suggests methodological rigor and clarity
3) The research has made substantial contributions to time series analysis
4) The framework allows for extension to diverse datasets and methodologies

### Steps in the Research
The replication encompasses the following key steps:

1. Replicating Engle's portfolio composition with identical asset allocation
2. Computing and evaluating portfolio return characteristics
3. Developing a custom GARCH(1,1) model implementation
4. Estimating GARCH(1,1) parameters using portfolio return data
5. Assessing the model's ability to capture volatility clustering patterns
6. Producing volatility forecasts following Engle's methodology
7. Implementing additional volatility models for comparative analysis
8. Analyzing portfolio performance using contemporary market data

This methodological approach enables a comprehensive evaluation of GARCH modeling techniques in financial time series analysis.


# Imports and Setup

```{python}
#| label: setup
import logging
import os
from datetime import datetime
import os, sys

#for mac
root = os.path.abspath(os.path.join(os.getcwd(), ".."))
if root not in sys.path:
    sys.path.insert(0, root)

print("Import paths:", sys.path[:3])
#####
import matplotlib.pyplot as plt
plt.style.use('seaborn-v0_8-darkgrid')
plt.rcParams['axes.grid'] = True

import numpy as np
import pandas as pd

if os.getcwd().endswith("src") or os.getcwd().endswith("report"):
    os.chdir("..")

from src.utils import *

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

```
# Data Preparation

## Portfolio Composition

The portfolio allocation follows Engle's specification:

- 50% allocation to Nasdaq Composite Index (^IXIC)
- 30% allocation to Dow Jones Industrial Average (^DJI)
- 20% allocation to 10-Year Treasury Constant Maturity Rate [^1]

[^1]: The ^TNX symbol is used as a proxy for the 10-year Treasury rate.

#### Data Source Considerations

Engle's original description states:

> Let's use the GARCH(1,1) tools to estimate the 1 percent
value at risk of a $1,000,000 portfolio on March 23, 2000. This portfolio consists of
50 percent Nasdaq, 30 percent DowJones and 20 percent long bonds. The long
bond is a ten-year constant maturity Treasury. The portfolio has constant proportions of wealth
in each asset that would entail some rebalancing over time.

The implementation requires several assumptions:

1. Daily rebalancing is assumed to maintain constant portfolio weights
2. The ^TNX symbol serves as a proxy for the 10-year Treasury rate
3. Portfolio returns are calculated using these constant weights

These assumptions may introduce some deviation from Engle's original analysis, but provide a reasonable approximation for replication purposes.

## Fetching Stock Data

We'll fetch the portfolio components data using the implemented function:

- The function is implemented in `src/utils.py`
- it uses `yfinance` - a Python package for downloading stock data from Yahoo Finance
- start and end dates are taken from the article
- weights for portfolio components are taken from the article, i.e. 50% Nasdaq, 30% Dow Jones, and 20% 10-year Treasury.

```{python}
#| label: fetch-data
# Parameters
# Nasdaq, Dow Jones, and 10-year Treasury
symbols = ["^IXIC", "^DJI", "^TNX"]  #
# Define the date range, based on the paper
# Sample period
start_date = datetime(1990, 3, 22)
end_date = datetime(2001, 6, 1)

# Portfolio weights, taken from the article
weights = {
    "^IXIC": 0.50,  # Nasdaq
    "^DJI": 0.30,  # Dow Jones
    "^TNX": 0.20,  # 10-year Treasury
}


# Fetch data using our implementation
logger.info("Fetching data...")
prices = fetch_stock_data(symbols, start_date, end_date)

# Display the first few rows
prices.tail()

# Apparently, DJIA data is missing for 1990-1992, so we'll drop it
prices = prices.drop(columns=["^DJI", "^TNX"])
```

### RATE and Dow Jones Industrial Average (^DJI)

As the data for `^DJI` was missing for 1990-1992, we had to take it from somewhere else.
We found Dow Jones Industrial Average (^DJI) data [here](https://www.kaggle.com/datasets/shiveshprakash/34-year-daily-stock-data).

For RATE we used a good proxy which we found here looks like a good proxy for TNX, we'll use data we found [here](https://fred.stlouisfed.org/data/DGS10)

####  DJIA data retrieval

```{python}
djia_prices = pd.read_csv(
    "./data/dja-performance-report-daily.csv",
    index_col="dt",
    parse_dates=True,
)
djia_prices = djia_prices.rename(columns={"Close Value": "^DJI"})
prices = prices.join(djia_prices["djia"])
prices.rename({'djia': '^DJI'}, axis=1, inplace=True)
```

####  RATE data retrieval

Data has been obtained from the hyperlink

```{python}
tnote_yield = pd.read_csv(
    "./data/DGS10.csv",
    index_col="observation_date",
    parse_dates=True,
)
tnote_yield = tnote_yield.rename(columns={"DGS10": "^TNX"})

# tnote yield is not exactly the price, but we merge it anyway
prices = prices.join(tnote_yield["^TNX"])
prices
```

## Calculating Returns

This function uses log returns, which are calculated as follows:

$$r_t = \ln \left( \frac{P_t}{P_{t-1}} \right)$$

where $P_t$ is the price at time $t$.

```{python}
#| label: calculate-returns
returns = calculate_returns(prices)

portfolio_returns = pd.Series(0, index=returns.index, dtype=float)

# Apply weights to each component
for symbol, weight in weights.items():
    # Use pandas multiplication and addition
    portfolio_returns += returns[symbol] * weight

# Add portfolio returns to the returns DataFrame
returns["portfolio"] = portfolio_returns
```

We split the data and proceed with sample for period 1990-2000. In the original paper out of sample forecast has been produced, thus we need additional year of data to replicate this.

```{python}
# split the data
returns_oos = returns.loc['2000-03-24':]
returns = returns.loc[:'2000-03-23']

prices_oos = prices.loc['2000-03-24':]
prices = prices.loc[:'2000-03-23']
```

```{python}
returns
```

## Visualizing Price and Returns

Let's visualize prices (excl. portfolio) and returns for all data series and compare with the plot from the article.

Visual inspection of the plot below shows that the data is very similar to the one in the article. The only difference is `^TNX` which doesn't overlap perfectly with plots from the article (could be easily spotted by looking at the outliers.)

Possible explanation:
 The 10-year Treasury Constant Maturity Rate (^TNX) is a model-derived value rather than an actual market price. Since new 10-year Treasury bonds aren't issued daily, the constant maturity yield represents a theoretical value of what a 10-year Treasury security would yield if issued at current market conditions. This value is calculated through interpolation of yields from Treasury securities with similar credit quality but different maturities. While this approximation closely tracks what an actual new 10-year bond would yield, small discrepancies can exist. Nevertheless, the constant maturity rate provides valuable insights into market expectations regarding inflation, economic growth, and future interest rates.

Hence, it's likely that Engle's methodology and / or selection of the securities with the same level of credit riskiness is different than the one used by Federal Reserve Bank of St. Louis in 2025.

```{python}
#| label: fig-prices
#| fig-cap: Nasdaq, Dow Jones and Bond Prices
#| fig-subcap:
#|   - 'Sample: March 23, 1990 to March 23, 2000.'

fig, axes = plt.subplots(3, 1, figsize=(12, 8))

axes[0].plot(prices['^DJI'], color='black')
axes[0].set_title('^DJI')

axes[1].plot(prices['^IXIC'], color='black')
axes[1].set_title('^IXIC')

axes[2].plot(prices['^TNX'], color='black')
axes[2].set_title('^TNX')

plt.show()
```

#### Returns from the original paper (for reference)
![Returns from the original paper](screenshots/returns.png)

```{python}
#| label: fig-returns
#| fig-cap: Nasdaq, Dow Jones and Bond Returns
#| fig-subcap:
#|   - 'Sample: March 23, 1990 to March 23, 2000.'
fig, axes = plt.subplots(2, 2, figsize = (16, 8))

axes[0, 0].plot(returns['^DJI'], color = "black")
axes[0, 0].set_title('^DJI')

axes[0, 1].plot(returns['^TNX'], color = "black")
axes[0, 1].set_title('^TNX')

axes[1, 0].plot(returns['^IXIC'], color = "black")
axes[1, 0].set_title('^IXIC')

axes[1, 1].plot(returns['portfolio'], color = "black")
axes[1, 1].set_title('portfolio')

plt.tight_layout()
plt.show()
```

## Portfolio Statistics

#### Article Portfolio Statistics (for reference)
![Portfolio Statistics](screenshots/portfolio_statistics.png)

```{python}
#| label: tbl-portfolio-stats
#| tbl-cap: Portfolio Statistics
#| tbl-cap-location: top
portfolio_stats = calculate_portfolio_stats(returns)
portfolio_stats
```

The table above shows the summary statistics for our portfolio components and the overall portfolio.
The statistics include mean returns, standard deviation, skewness, and kurtosis for each component.

Our results show minor differences compared to Table 1 in Engle (2001):

* The statistics for Nasdaq (^IXIC) and Dow Jones (^DJI) differ by only 1-2 basis points
* Larger discrepancies appear for the Rate component, likely because the article lacks specificity about the exact data source used
    + The most notable differences are in skewness (0.38 vs -0.20) and kurtosis (4.96 vs 5.96)
* These differences in the Rate component are the primary reason for the slight variation between our replicated portfolio and the one presented in the paper

## ACF of Squared Portfolio Returns

#### Article ACF of Squared Portfolio Returns (for reference)
![ACF of Squared Portfolio Returns](screenshots/squared_returns.png)

```{python}
#| label: tbl-portfolio-returns
#| tbl-cap: Autocorrelations of Squared Portfolio Returns
acf_plot = calculate_acf_table(returns["portfolio"])
acf_plot
```

# GARCH Model

ARCH models were first introduced by Engle in 1982 to model time-varying volatility in economic and financial time series. The idea was as follows - high squared residual at time $t$ leads to increased conditional variance at time $t+p$. We can write such ARCH(p) process:
$$
\begin{cases}
r_t = \mu_t + \varepsilon_t \quad \varepsilon_t \sim \mathcal{N}(0, \sigma_t^2) \\
\sigma_t^2 = \omega + \alpha_1 \varepsilon_{t-1}^2 + \alpha_2 \varepsilon_{t-2}^2 + ... + \alpha_p \varepsilon_{t-p}^2
\end{cases}
$$
However as shocks in time series often tend to be highly persistent, proper ARCH specification required quite big number of lags p.

This lead to obvious extension that includes past values of conditional variance. Such solution was proposed by Bollerslev in 1986 and is known as GARCH model. Interestingly Bollerslev was a student of Engle, who supervised his doctoral work.

GARCH(p,q) model is defined as follows:
$$
\begin{cases}
r_t = \mu_t + \varepsilon_t, \quad \varepsilon_t \sim \mathcal{N}(0, \sigma_t^2) \\
\sigma_t^2 = \omega + \sum_{i=1}^p \alpha_i \varepsilon_{t-i}^2 + \sum_{j=1}^q \beta_j \sigma_{t-j}^2
\end{cases}
$$
We estimate model parameters using maximum likelihood estimator. Assuming conditional normality we define likelihood function as follows:
$$
L(\theta) = \prod_{t=1}^T \frac{1}{\sqrt{2\pi \sigma_t^2}} \exp\left( -\frac{(r_t - \mu_t)^2}{2 \sigma_t^2} \right)
$$

## GARCH(1,1)

The GARCH(1,1) model is specified as:

$$\sigma_t^2 = \omega + \alpha_1 \varepsilon_{t-1}^2 + \beta_1 \sigma_{t-1}^2$$

where:

- $\sigma_t^2$ is the conditional variance
- $\omega$ is the constant term
- $\alpha_1$ is the ARCH effect
- $\beta_1$ is the GARCH effect
- $\varepsilon_{t-1}^2$ is the squared lagged returns
- $\sigma_{t-1}^2$ is the lagged conditional variance

We apply procedure to estimate GARCH(1,1) parameters

```{python}
# Define initial parameters
r = returns['portfolio']*1000
mu = np.mean(r)
resid = r - mu
resid_sq = resid**2


# Log likelihood
def negative_loglik(params):
    omega, alpha, beta = params
    sigma2 = np.zeros(len(r))

    # Calculate conditional variance
    sigma2[0] = omega/(1-alpha-beta)

    for t in range(1, len(sigma2)):
        sigma2[t] = omega + alpha*resid_sq[t-1] + beta*sigma2[t-1]


    logl = np.log(1/(np.sqrt(2*np.pi*sigma2))*np.exp(-resid_sq/(2*sigma2)))
    #logl = -0.5 * (np.log(2 * np.pi) + np.log(sigma2) + resid_sq / sigma2)
    sum_logl = np.sum(logl)

    return -sum_logl
```

We do not write our own optimizing function as it is out of the scope of this project. Instead we use existing implementation from scipy

```{python}
# We want to maximize sum of log likelihoods
from scipy.optimize import minimize

initial_guess = [np.var(r), 0, 0]
bounds = [(0, None), (0, None), (0, None)]

def constraint_stationarity(params):
    _, alpha, beta = params
    return 1 - alpha - beta

constraints = ({
    'type': 'ineq',
    'fun': constraint_stationarity
})

optimal_res = minimize(
    negative_loglik,
    initial_guess,
    bounds=bounds,
    constraints=constraints,
    options={'maxiter' : 1000}
)
print('Success:', optimal_res.success)
print('Message:', optimal_res.message)
print('n iterations', optimal_res.nit)
```

```{python}
from tabulate import tabulate

omega, alpha, beta = optimal_res.x

table = [
    ['omega', omega],
    ['alpha', alpha],
    ['beta', beta]
]

print(tabulate(table, headers=['Parameter', 'Value'], floatfmt=".6f"))
```

```{python}
# Conditional volatility
sigma2 = np.zeros(len(r))

# Calculate conditional variance
sigma2[0] = omega/(1-alpha-beta)

for t in range(1, len(sigma2)):
    sigma2[t] = omega + alpha*resid_sq[t-1] + beta*sigma2[t-1]


plt.figure(figsize=(12, 6))
plt.plot(np.sqrt((r.values/100)**2), color='black', lw=0.9)
plt.plot(np.sqrt(sigma2/10000), color='red', lw=1.1)
plt.title('Custom GARCH(1,1) conditional volatility')
plt.show()
```

## GARCH Model Existing Implementation

Above we have made our own implementation of GARCH(1,1) model. However there are already existing implementations. We will proceed with the study using arch_model from arch library.

#### Article GARCH(1,1) Model (for reference)
![GARCH(1,1) Model](screenshots/garch_one_one.png)

We use arch library to fit GARCH(1, 1) model. However fitting the model to our returns in the original scale produces warning with recommendation to scale the returns by multiplying them by 1000. Therefore we fit the model to scaled returns. Here we have to remember that forecasted conditional volatility obtained from the resulting object will be scaled as well. For visuals we will rescale back to the original scale.

```{python}
#| label: tbl-garch-one-one
#| tbl-cap: GARCH(1,1)
#| tbl-cap-location: top

logger.info("Fitting GARCH(1,1) model...")
results = fit_garch(returns["portfolio"]*1000, vol="Garch", dist="normal")

#logger.info(results.summary())
# Extract coefficients, standard errors, z-statistics, and p-values
coef = results.params
std_err = results.std_err
z_stat = coef / std_err
p_values = results.pvalues

# Create a DataFrame to display the results
model_results = pd.DataFrame(
    {
        "Coef": coef,
        "St. Err": std_err,
        "Z-Stat": z_stat,
        "P-Value": p_values,
    }
)

model_results
```

## ACF of Squared Standardized Residuals

As in the previous analysis, we compute autocorrelations up to lag 15, including Q-statistics and their corresponding p-values. The results closely align with those reported in the original paper. The standardized residuals represent returns adjusted for GARCH effects, given that the mean portfolio return is approximately zero. This interpretation suggests that the GARCH(1,1) model effectively captures the variance dynamics in portfolio returns.

#### Article ACF of Squared Standardized Residuals (for reference)
![ACF of Squared Standardized Residuals](screenshots/squared_standardized_residuals.png)

```{python}
#| label: tbl-squared-residuals
#| tbl-cap: Autocorrelations of Squared Standardized Residuals
acf_plot = calculate_acf_table(results.resid / results.conditional_volatility)
acf_plot
```

# Volatility Analysis

## Conditional Volatility

Let's plot the conditional volatility using our implementation:

```{python}
#| label: fig-conditional-volatility
#| fig-cap: Conditional Volatility

plt.figure(figsize=(12, 6))
plt.plot(results.conditional_volatility/1000, color='black')
plt.title('Conditional volatility')
plt.show()
```

## Model Diagnostics

Let's examine the model residuals:

```{python}
#| label: model-diagnostics
#| fig-cap: Standardized Residuals

# Get standardized residuals
std_resid = (results.resid / np.sqrt(results.conditional_volatility))/1000

plt.figure(figsize=(12, 6))
plt.plot(std_resid, color='black', lw=1)
plt.title("Standardized residuals")
plt.show()
```

# Volatility Forecasting

## Generating Forecasts

Let's generate volatility forecasts:

```{python}
#| label: forecast
# Generate forecasts
logger.info("Generating volatility forecast...")
forecast = results.forecast(horizon=5)
logger.info("\nVolatility Forecast:")
logger.info(np.sqrt(forecast.variance.iloc[-1])/1000)
```

## Visualizing Forecasts

Visual below shows conditional volatility and realized volatility defined as square root of squared returns assuming 0 mean:
$$
\text{Var}(r_t) = \mathbb{E}[(r_t - \mu)^2]
$$
if $\mu = 0$ we get:
$$
\text{Var}(r_t) = \mathbb{E}[r_t^2]
$$

```{python}
#| label: fig-forecast
#| fig-cap: Volatility Forecast

# Plot volatility
plt.figure(figsize = (14, 8))
plt.plot(np.sqrt(returns['portfolio']**2), color = 'black', lw = 0.9)
plt.plot(results.conditional_volatility/1000, color = 'red', lw = 1.1)
plt.title("Volatility realized vs predicted")
plt.show()
```

At this point we can also compare results from our implementation with those obtained using arch library implementation

```{python}
plt.figure(figsize = (14, 8))
plt.plot(np.sqrt(returns['portfolio']**2), color = 'black', lw = 0.9, label = 'realized volatility')
plt.plot(r.index, np.sqrt(sigma2/1000000), color = 'green', lw = 1.1, label = 'GARCH - our implementation')
plt.plot(results.conditional_volatility/1000, color = 'red', lw = 1.1, label = 'GARCH - arch library')
plt.title("Volatility realized vs predicted")
plt.legend()
plt.show()
```

The green line showing our GARCH model is hard to see because it almost perfectly matches the red line from the arch library. This means our model gives very similar results to the standard library version.

# Value at Risk
Based on the estimated conditional volatility, we calculate the 1% Value at Risk (VaR) for a portfolio with an initial value of $1,000,000. The one-step-ahead forecast yields a standard deviation of 0.014772.

We assume returns follow a normal distribution. The 1% quantile is calculated by multiplying the predicted standard deviation by 2.327, which represents the critical value for a 99% confidence interval under normality.

```{python}
init_value = 10**6
2.327*np.sqrt(forecast.variance['h.1'].iloc[-1])/1000 * init_value
```
The 1% Value at Risk (VaR) under normal distribution assumptions is $34,375.
Since the mean return is negligible, we can exclude it from our calculations.
The VaR represents the 99th percentile of the distribution, which is
symmetric around zero due to the near-zero mean.

An alternative approach is to use the empirical 1% quantile.

```{python}
emp_q = np.quantile(results.resid/results.conditional_volatility, 0.01)
print(f"1 percent quantile: {emp_q}")
print(-emp_q*np.sqrt(forecast.variance['h.1'].iloc[-1])/1000 * init_value)
```

The results we obtained are pretty similar to those presented in the original paper. Interestingly, even the empirical 1st quantile is very close to -2.844, which is the original result.

We also plot 1% value at risk for the whole in sample period. For reference, we show the same plot from the original study.

![in-sample-VaR](screenshots/in_sample_VaR.png)

```{python}
#| label: fig-portfolio_loss
#| fig-cap: Portfolio loss in sample

# Calculate VaR
VaR_returns = calculate_VaR_returns(returns["portfolio"], results.conditional_volatility/1000, 2.327)

# Value at risk - portfolio value 1 000 000$ at each point in time
init_value = 10**6
portfolio_returns = returns['portfolio']*init_value
portfolio_VaR = VaR_returns*init_value

plt.figure(figsize = (14, 8))
plt.plot(-portfolio_returns, color = 'black', lw = 1, label='Loss')
plt.plot(-portfolio_VaR, color = 'red', lw = 0.9, label='VaR')
plt.title("Portfolio loss and 1% Value at Risk in sample")
plt.legend()
plt.show()
```

We also show the number of exceedances. A good model should produce VaR estimates where the actual portfolio loss exceeds the estimated value approximately 1% of the time. For our GARCH(1,1) model with VaR estimates assuming normal distribution of standardized residuals, the exceedance rate is 2.06%.

```{python}
# Exceedances
n_exc, perc_exc = count_exceedances(VaR_returns, returns["portfolio"], True)
print(f"Number of exceedances: {n_exc}")
print(f"Percentge of exceedances: {perc_exc*100:.2f}%")
```

# Out-of-sample model fit and Value at Risk

We obtain oit of sample predictions using rolling forecast without reestimating GARCH parameters

![oos-VaR](screenshots/out_of_sample_VaR.png)

```{python}
# Rolling forecast without reestimating parameters
mu = results.params['mu']
omega = results.params['omega']
alpha = results.params['alpha[1]']
beta = results.params['beta[1]']

epsilon2 = (returns_oos['portfolio']*1000 - mu)**2
sigma2 = np.zeros(len(returns_oos))
sigma2[0] = omega/(1 - alpha - beta)

for t in range(1, len(sigma2)):
    sigma2[t] = omega + alpha*epsilon2[t-1] + beta*sigma2[t-1]


plt.figure(figsize = (14, 8))
plt.plot(np.sqrt(returns_oos['portfolio']**2), color = 'black', lw = 1)
plt.plot(returns_oos.index, np.sqrt(sigma2)/1000, color = 'red', lw = 0.9)
plt.show()
```

```{python}
# Value at risk - portfolio value 1 000 000$ at each point in time
portfolio_returns = returns_oos['portfolio']*(10**6)

plt.figure(figsize = (14, 8))
plt.plot(-portfolio_returns, color = 'black', lw = 1)
plt.plot(returns_oos.index, np.sqrt(sigma2) * 2.327 * 10**3, color = 'red', lw = 0.9)
plt.title("Portfolio loss and 1% Value at Risk")
plt.show()
```

```{python}
n_exc, perc_exc = count_exceedances(np.sqrt(sigma2) * (-2.327)/1000, returns_oos['portfolio'].values, True)
print(f"Number of exceedances: {n_exc}")
print(f"Percentge of exceedances: {perc_exc*100:.2f}%")
```

# Extensions

## Alternative Error Distributions

```{python}
import arch
# 6.1   Estimate GARCH(1,1) with four innovation distributions
distros = {
    "Normal"   : "normal",
    "Student-t": "t",
    "Skew-t"   : "skewt",
    "GED"      : "ged",
}

alt_models = {}
for name, dist in distros.items():
    logger.info(f"Fitting GARCH(1,1) with {name} errors …")
    am = arch.arch_model(
        returns["portfolio"] * 1000,
        vol="GARCH",
        p=1,
        q=1,
        dist=dist,
        rescale=False,
    )
    res = am.fit(disp="off")
    alt_models[name] = res
```

```{python}
# 6.2   Information-criterion comparison table
comp = pd.DataFrame(
    {
        "Distribution": list(alt_models.keys()),
        "LogLik"      : [m.loglikelihood for m in alt_models.values()],
        "AIC"         : [m.aic            for m in alt_models.values()],
        "BIC"         : [m.bic            for m in alt_models.values()],
    }
).sort_values("AIC")
logger.info("\nError-Distribution Comparison:\n%s", comp.to_string(index=False))
```

```{python}
# 6.3  Overlay conditional-volatility paths

import matplotlib.pyplot as plt
import numpy as np

fig, ax = plt.subplots(figsize=(10, 6))

for name, res in alt_models.items():
    ax.plot(
        res.conditional_volatility.index,
        np.sqrt(res.conditional_volatility),
        label=name,
        linewidth=1,
    )

ax.set_title("Conditional Volatility – Alternative Error Distributions")
ax.set_xlabel("Date")
ax.set_ylabel("Volatility")
ax.legend(frameon=False)
fig.tight_layout()
plt.show()
```

```{python}
# 6.4   Q-Q plots of standardized residuals
import matplotlib.pyplot as plt
from scipy import stats

fig, axes = plt.subplots(2, 2, figsize=(12, 10))
axes = axes.flatten()

for ax, (name, res) in zip(axes, alt_models.items()):
    std_resid = res.resid / np.sqrt(res.conditional_volatility)
    if name == "Student-t":
        nu = res.params["nu"]
        stats.probplot(std_resid, dist="t", sparams=(nu,), plot=ax)
    else:  # Normal, Skew-t, GED → compare to N(0,1)
        stats.probplot(std_resid, dist="norm", plot=ax)
    ax.set_title(f"{name} errors")

fig.tight_layout()
plt.show()
```

We extend our study with E-GARCH and GJR-GARCH models.

## E-GARCH
The GARCH model is not perfect. One of its main drawbacks is that it assumes symmetrical volatility shocks, while in reality this assumption is usually not met. E-GARCH extends the standard GARCH model and accounts for possible non-symmetric volatility.

The conditional variance equation is:

$$
\ln(\sigma_t^2) = \omega + \beta \ln(\sigma_{t-1}^2) + \alpha \left| \frac{\varepsilon_{t-1}}{\sigma_{t-1}} \right| + \gamma \frac{\varepsilon_{t-1}}{\sigma_{t-1}}
$$

Where:

- $\sigma_t^2$: Conditional variance at time $t$
- $\varepsilon_t$: Residuals (innovations)
- $\omega: Constant term
- $\beta$: Persistence parameter
- $\alpha: Response to magnitude of shocks
- $\gamma$: Captures **asymmetry** (leverage effect)

```{python}
results_egarch = fit_garch(returns["portfolio"]*1000, vol="egarch", dist="normal")

# Extract coefficients, standard errors, z-statistics, and p-values
coef = results_egarch.params
std_err = results_egarch.std_err
z_stat = coef / std_err
p_values = results_egarch.pvalues

# Create a DataFrame to display the results
model_results = pd.DataFrame(
    {
        "Coef": coef,
        "St. Err": std_err,
        "Z-Stat": z_stat,
        "P-Value": p_values,
    }
)

model_results
```

```{python}
VaR_returns = calculate_VaR_returns(returns["portfolio"], results_egarch.conditional_volatility/1000, 2.327)

# Value at risk - portfolio value 1 000 000$ at each point in time
init_value = 10**6
portfolio_returns = returns['portfolio']*init_value
portfolio_VaR = VaR_returns*init_value

plt.figure(figsize = (14, 8))
plt.plot(-portfolio_returns, color = 'black', lw = 1, label='Loss')
plt.plot(-portfolio_VaR, color = 'red', lw = 0.9, label='VaR')
plt.title("Portfolio loss and 1% Value at Risk in sample")
plt.legend()
plt.show()
```

```{python}
# Exceedances
n_exc, perc_exc = count_exceedances(VaR_returns, returns["portfolio"], True)
print(f"Number of exceedances: {n_exc}")
print(f"Percentge of exceedances: {perc_exc*100:.2f}%")
```

## GJR-GARCH

Another model relaxing the symmetric volatility shocks assumption is GJR-GARCH

Let the return be defined as:

$$
r_t = \mu + \varepsilon_t \\
\varepsilon_t = \sigma_t z_t
$$

The conditional variance equation of GJR-GARCH(1,1,1) is:

$$
\sigma_t^2 = \omega + \alpha \varepsilon_{t-1}^2 + \gamma \varepsilon_{t-1}^2 \cdot I_{\{\varepsilon_{t-1} < 0\}} + \beta \sigma_{t-1}^2
$$

Where:

- $ r_t$: Return at time  $t$
- $\varepsilon_t$: Innovation (shock)
- $\sigma_t^2$: Conditional variance
- $z_t \sim N(0,1)$: i.i.d. standard normal variable
- $\omega > 0$: Constant term
- $\alpha \geq 0$: ARCH coefficient
- $\beta \geq 0$: GARCH coefficient
- $\gamma \geq 0$: Leverage/asymmetry coefficient
- $I_{\{\varepsilon_{t-1} < 0\}}$: Indicator function (1 if $\varepsilon_{t-1} < 0$, else 0)

```{python}
results_gjrgarch = fit_garch(returns["portfolio"]*1000, o=1, dist="normal")

# Extract coefficients, standard errors, z-statistics, and p-values
coef = results_gjrgarch.params
std_err = results_gjrgarch.std_err
z_stat = coef / std_err
p_values = results_gjrgarch.pvalues

# Create a DataFrame to display the results
model_results = pd.DataFrame(
    {
        "Coef": coef,
        "St. Err": std_err,
        "Z-Stat": z_stat,
        "P-Value": p_values,
    }
)

model_results
```

```{python}
fitted = {
    "garch" : results,
    "e-garch" : results_egarch,
    "gjr-garch" : results_gjrgarch
}

param_frames = []
for name, res in fitted.items():
    df = res.params.rename(name).to_frame().T
    param_frames.append(df)
comparison = pd.concat(param_frames)
print(comparison.round(4))
```

```{python}
VaR_returns = calculate_VaR_returns(returns["portfolio"], results_gjrgarch.conditional_volatility/1000, 2.327)

# Value at risk - portfolio value 1 000 000$ at each point in time
init_value = 10**6
portfolio_returns = returns['portfolio']*init_value
portfolio_VaR = VaR_returns*init_value

plt.figure(figsize = (14, 8))
plt.plot(-portfolio_returns, color = 'black', lw = 1, label='Loss')
plt.plot(-portfolio_VaR, color = 'red', lw = 0.9, label='VaR')
plt.title("Portfolio loss and 1% Value at Risk in sample")
plt.legend()
plt.show()
```

```{python}
# Exceedances
n_exc, perc_exc = count_exceedances(VaR_returns, returns["portfolio"], True)
print(f"Number of exceedances: {n_exc}")
print(f"Percentge of exceedances: {perc_exc*100:.2f}%")
```

```{python}
plt.figure(figsize=(14, 8))
plt.plot(np.sqrt(returns["portfolio"]**2), color='black', lw=0.9, label='returns')
plt.plot(results.conditional_volatility/1000, color='red', lw=1.1, label='garch')
plt.plot(results_egarch.conditional_volatility/1000, color='green', lw=1.1, label='e-garch')
plt.plot(results_gjrgarch.conditional_volatility/1000, color='blue', lw=1.1, label='gjr-garch')
plt.title("Conditional Volatility Comparison")
plt.legend()
plt.show()
```

The plot above shows that predicted in-sample volatility is similar for all three models. The E-GARCH model gives the least number of exceedances for in-sample VaR.

# Modern Market

We check how GARCH(1,1) performs for more recent data, particularly the 2015-2025 period.

```{python}
start_date = datetime(2015, 1, 1)
end_date = datetime(2025, 1, 1)

prices_new = fetch_stock_data(symbols, start_date, end_date)
returns_new = calculate_returns(prices_new)

portfolio_returns_new = pd.Series(0, index=returns_new.index, dtype=float)

# Apply weights to each component
for symbol, weight in weights.items():
    # Use pandas multiplication and addition
    portfolio_returns_new += returns_new[symbol] * weight

# Add portfolio returns to the returns DataFrame
returns_new["portfolio"] = portfolio_returns_new


acf_plot = calculate_acf_table(returns_new["portfolio"])
acf_plot
```

The ACF table reveals that squared returns exhibit stronger correlations in the current dataset compared to the original study's data.

```{python}
garch_new = results = fit_garch(returns_new["portfolio"]*1000, vol="Garch", dist="normal")
coef = garch_new.params
std_err = garch_new.std_err
z_stat = coef / std_err
p_values = garch_new.pvalues

# Create a DataFrame to display the results
model_results = pd.DataFrame(
    {
        "Coef": coef,
        "St. Err": std_err,
        "Z-Stat": z_stat,
        "P-Value": p_values,
    }
)

model_results
```

```{python}
plt.figure(figsize = (14, 8))
plt.plot(np.sqrt(returns_new['portfolio']**2), color = 'black', lw = 0.9, label = 'realized volatility')
plt.plot(garch_new.conditional_volatility/1000, color = 'red', lw = 1.1, label = 'conditional volatility')
plt.title("Volatility realized vs predicted")
plt.legend()
plt.show()
```

The conditional volatility appears well-fitted, demonstrating that the GARCH model performs effectively even in contemporary markets. We observe a significant volatility spike during the COVID-19 period, which our model substantially underestimates.

```{python}
# Value at risk - portfolio value 1 000 000$ at each point in time
VaR_returns = calculate_VaR_returns(returns_new["portfolio"], garch_new.conditional_volatility/1000, 2.327)

init_value = 10**6
portfolio_returns = returns_new['portfolio']*init_value
portfolio_VaR = VaR_returns*init_value

plt.figure(figsize = (14, 8))
plt.plot(-portfolio_returns, color = 'black', lw = 1, label='Loss')
plt.plot(-portfolio_VaR, color = 'red', lw = 0.9, label='VaR')
plt.title("Portfolio loss and 1% Value at Risk in sample")
plt.legend()
plt.show()
```

```{python}
n_exc, perc_exc = count_exceedances(VaR_returns, returns_new["portfolio"], True)
print(f"Number of exceedances: {n_exc}")
print(f"Percentge of exceedances: {perc_exc*100:.2f}%")
```

# Conclusion

* Engle, R. F. (2001). GARCH 101: The Use of ARCH/GARCH Models in Applied Econometrics. Journal of Economic Perspectives, 15(4), 157-168.
* Kaggle. (2023). Dow Jones Industrial Average (^DJI) Data. [Dataset]. Available at: https://www.kaggle.com/datasets/shiveshprakash/34-year-daily-stock-data
* St. Louis Fed. (2023). 10-Year Treasury Constant Maturity Rate [^TNX]. [Dataset]. Available at: https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23ebf3fb&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1320&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=DGS10&scale=left&cosd=2020-04-17&coed=2025-04-17&line_color=%230073e6&link_values=false&line_style=solid&mark_type=none&mw=3&lw=3&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily&fam=avg&fgst=liin&fgsnd=2020-02-01&line_index=1&transformation=lin&vintage_date=2025-04-22&revision_date=2025-04-22&nd=1962-01-02
* Yahoo Finance. (2023). [Dataset]. Available at: https://finance.yahoo.com/quote/%5EIXIC/history?period1=631152000&period2=1713878400&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true
* GenAI used for proof checking and type hinting
